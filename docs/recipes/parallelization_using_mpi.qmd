---
title: "Parallelization using Open MPI"
---

On this page, we provide a step-by-step recipe on how you can start parallelize your code using the "mpi4py" library from Open MPI program.


# Embarrassingly parallelization of Spool's Patches over processors

## Install DASCore and mpi4py libraries

First, make sure you have installed DASCore on your machine. See [DASCore Installation](https://dascore.org/#:~:text=0.2%29%3B-,Installation). Secondly, you need to properly install the mpi4py library. After installing and loading the [Open MPI](https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html) module on your machine (e.g., on Linux: ` load module to/mpi/openmpi/gcc/compiler/path`), install [mpi4py](https://pypi.org/project/mpi4py/). It might be easier to install using conda-forge as below:

```bash
conda install -c conda-forge mpi4py openmpi
```

Please note that this procedure is tested for Python 3.11 and Open MPI GCC 3.1.3


## Parallelize using DASCore and mpi4py

Here is an example for parallelization of Patches over processors:

```python
import dascore as dc

from mpi4py import MPI


# Load the Spool
sp = dc.get_example_spool("random_das")

# Initiate MPI
comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

if rank==0: # For the first processor
    splits = len(sp)
    if splits<1:
        raise ValueError('No Patch of data found within the Spool (%s)'%(sp))
else:
    splits = sp = None

# Broadcast the variables
splits = comm.bcast(splits, root=0)
sp = comm.bcast(sp, root=0)

for i in range(rank, splits, size):
    print("loop (i): ", i, ", processor: ", rank, ", patch: ", sp[i])
    patch = sp[i]
    ...

```
