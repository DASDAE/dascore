[
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "DasCore",
    "section": "",
    "text": "Installation\nDASCore can be installed with pip:\n\npip install dascore\nor conda:\n\nconda install dascore -c conda-forge\nSee contributing for developer installation."
  },
  {
    "objectID": "markdown/support.html",
    "href": "markdown/support.html",
    "title": "DasCore",
    "section": "",
    "text": "Support\nThe DASCore Github page is the place where DASCore development, support and discussion takes place. If you encounter and issue, discover a bug, or want to ask a question please first search the issues to see if your issue has already been addressed. If not, file a new issue by clicking on the green “New Issue” button and selecting the appropriate issue category.\nIf you want to discuss new features or propose changes to DASCore then the discussion tab is the right place to start."
  },
  {
    "objectID": "introduction/intro.html",
    "href": "introduction/intro.html",
    "title": "DasCore",
    "section": "",
    "text": "A python library for distributed fiber optic sensing."
  },
  {
    "objectID": "introduction/intro.html#read-a-single-file",
    "href": "introduction/intro.html#read-a-single-file",
    "title": "DasCore",
    "section": "Read a single file",
    "text": "Read a single file\nimport dascore as dc\nfrom dascore.utils.downloader import fetch\n\n# get a path to an example file, replace with your path\nfile_path = fetch('terra15_das_1_trimmed.hdf5')\n\nspool = dc.spool(file_path)"
  },
  {
    "objectID": "introduction/intro.html#read-data-from-a-directory-of-fiber-files",
    "href": "introduction/intro.html#read-data-from-a-directory-of-fiber-files",
    "title": "DasCore",
    "section": "Read data from a directory of fiber files",
    "text": "Read data from a directory of fiber files\nimport dascore as dc\nfrom dascore.utils.downloader import fetch\n\n# get a path to a directory of das files, replace with your path\ndirectory_path = fetch('terra15_das_1_trimmed.hdf5').parent\n\nspool = dc.spool(directory_path).update()"
  },
  {
    "objectID": "introduction/intro.html#get-patches-2d-array",
    "href": "introduction/intro.html#get-patches-2d-array",
    "title": "DasCore",
    "section": "Get patches (2D array)",
    "text": "Get patches (2D array)\nimport dascore as dc\nspool = dc.get_example_spool('diverse_das')\n\n# get patches through iteration\nfor patch in spool:\n    ...\n\n# Or through indexing\npatch = spool[0]"
  },
  {
    "objectID": "introduction/intro.html#perform-processing",
    "href": "introduction/intro.html#perform-processing",
    "title": "DasCore",
    "section": "Perform processing",
    "text": "Perform processing\nimport dascore as dc\npatch = dc.get_example_spool('random')\n\nout = (\n    patch.decimate(time=8)  # decimate along time axis\n    .detrend(dim='distance')  # detrend in distance axis\n    .pass_filter(time=(None, 10))  # apply bandpass filter\n)"
  },
  {
    "objectID": "introduction/intro.html#visualize",
    "href": "introduction/intro.html#visualize",
    "title": "DasCore",
    "section": "Visualize",
    "text": "Visualize\nimport dascore as dc\npatch = dc.get_example_spool('random')\n\npatch.viz.waterfall(show=True, scale=0.02)"
  },
  {
    "objectID": "introduction/quickstart.html",
    "href": "introduction/quickstart.html",
    "title": "DasCore",
    "section": "",
    "text": "A quickstart for DASCore, a python library for fiber-optic sensing.\nDASCore has two main data structures; the Patch and the Spool.\n\n\nA section of contiguous, uniformly sampled data is called a Patch. These can be generated in a few ways:\n\n\nimport dascore as dc\npa = dc.get_example_patch()\n\n\n\nWe first download an example fiber file (you need an internet connection). Next, we simply read it into a spool object then get the first (and only) patch.\n# get a fiber file\nimport dascore as dc\nfrom dascore.utils.downloader import fetch\npath = fetch(\"terra15_das_1_trimmed.hdf5\")  # path to a datafile\n\npa = dc.read(path)[0]\n\n\n\nPatches can also be created from numpy arrays and dictionaries. This requires:\n\nThe data array\nThe coordinates for labeling each axis\nThe attributes (optional)\n\nimport numpy as np\n\nimport dascore as dc\nfrom dascore.utils.time import to_timedelta64\n\n\n# Create the patch data\narray = np.random.random(size=(300, 2_000))\n\n# Create attributes, or metadata\nt1 = np.datetime64(\"2017-09-18\")\nattrs = dict(\n    d_distance=1,\n    d_time=to_timedelta64(1 / 250),\n    category=\"DAS\",\n    id=\"test_data1\",\n    time_min=t1,\n)\n\n# Create coordinates, labels for each axis in the array.\ncoords = dict(\n    distance=np.arange(array.shape[0]) * attrs[\"d_distance\"],\n    time=np.arange(array.shape[1]) * attrs[\"d_time\"],\n)\npa = dc.Patch(data=array, coords=coords, attrs=attrs)\nprint(pa)\n\n\n\n\nFor various reasons, Patches should be treated as immutable, meaning they should not be modified in place, but rather new patches created when something needs to be modified.\nThe patch has several methods which are intended to be chained together via a fluent interface, meaning each method returns a new Patch instance.\nimport dascore as dc\npa = dc.get_example_patch()\n\nout = (\n    pa.decimate(time=8)  # decimate to reduce data volume by 8 along time dimension\n    .detrend(dim='distance')  # detrend along distance dimension\n    .pass_filter(time=(None, 10))  # apply a low-pass 10 Hz butterworth filter\n)\nThe processing methods are located in the dascore.proc package.\n\n\n\nDASCore provides various visualization functions found in the dascore.viz package or using the Patch.viz namespace.\nimport dascore as dc\npa = dc.get_example_patch()\npa.viz.waterfall(show=True)\n\n\n\nBecause patches should be treated as immutable objects, you can’t just modify them with normal item assignment. There are a few methods that return new patches with modifications, however, that are functionally the same.\n\n\nOften you may wish to modify one aspect of the patch. Patch.new is designed for this purpose:\nimport dascore as dc\npa = dc.get_example_patch()\n\n# create a copy of patch with new data but coords and attrs stay the same\nnew = pa.new(data=pa.data * 10)\n\n\n\nPatch.update_attrs() is for making small changes to the patch attrs (metadata) while keeping the unaffected metadata (Patch.new would require you replace the entirety of attrs).\nimport dascore as dc\npa = dc.get_example_patch()\n\n# update existing attribute 'network' and create new attr 'new_attr'\npa1 = pa.update_attrs(**{'network': 'experiment_1', 'new_attr': 42})\nPatch.update_attrs also tries to keep the patch attributes consistent. For example, changing the start, end, or sampling of a dimension should update the other attributes affected by the change.\nimport dascore as dc\npa = dc.get_example_patch()\n\n# update start time should also shift endtime\npa1 = pa.update_attrs(time_min='2000-01-01')\nprint(pa.attrs['time_min'])\nprint(pa1.attrs['time_min'])\n\n\n\n\nSpools are containers/managers of patches. These come in a few varieties which can manage a group of patches loaded into memory, archives of local files, and (in the future) a variety of clients for accessing remote resources.\nThe simplest way to get the appropriate spool for a specified input is to use the load method, which should just work in the vast majority of cases.\nimport dascore as dc\n\n# create a list of patches\npatch_list = [dc.get_example_patch()]\n\n# get a spool for managing in-memory patches\nspool1 = dc.spool(patch_list)\n\n# get a spool from a das file\nfrom dascore.utils.downloader import fetch\npath_to_das_file = fetch(\"terra15_das_1_trimmed.hdf5\")\nspool2 = dc.spool(path_to_das_file)\n\n# get a spool from a directory of DAS files\ndirectory_path = path_to_das_file.parent\nspool3 = dc.spool(directory_path)\nDespite some implementation differences, all spools have common behavior/methods.\n\n\nPatches are extracted from the spool via simple iteration or indexing. New spools are returned via slicing.\nimport dascore as dc\nspool = dc.get_example_spool()\n\npatch = spool[0]  # extract first patch\n\n# iterate patchs\nfor patch in spool:\n    ...\n\n# slice spool to create new spool which excludes first patch.\nnew_spool = spool[1:]\n\n\n\nReturns a dataframe listing contents. This method may not be supported on all spools, especially those interfacing with vast remote resources.\nimport dascore as dc\nspool = dc.get_example_spool()\n\n# Return dataframe with contents of spool (each row has metadata of a patch)\nprint(spool.get_contents())\n\n\n\nSelects a subset of spool and returns a new spool. get_contents will now reflect subset of the original data requested by the select operation.\nimport dascore as dc\nspool = dc.get_example_spool()\n\n# select a spool with\nsubspool = spool.select(time=('2020-01-03T00:00:09', None))\nIn addition to trimming the data along a specified dimension (as shown above), select can be used to filter patches that meet a specified criteria.\nimport dascore as dc\n# load a spool which has many diverse patches\nspool = dc.get_example_spool('diverse_das')\n\n# Only include patches which are in network 'das2' or 'das3'\nsubspool = spool.select(network={'das2', 'das3'})\n\n# only include spools which match some unix-style query on their tags.\nsubspool = spool.select(tag='some*')\n\n\n\nChunk controls how data are grouped together in patches. It can be used to merge contiguous patches together, specify size of patches for processing, overlap with previous segments, etc.\nimport dascore as dc\nspool = dc.get_example_spool()\n\n# chunk spool for 3 second increments with 1 second overlaps\n# and keep any segements that don't have full 3600 seconds\nsubspool = spool.chunk(time=3, overlap=1, keep_partial=True)\n\n# merge all contiguous segments along time dimension\nmerged_spool = spool.chunk(time=None)"
  },
  {
    "objectID": "introduction/working_with_files.html",
    "href": "introduction/working_with_files.html",
    "title": "DasCore",
    "section": "",
    "text": "DASCore contains two data structures which are useful for working with directories of fiber data.\n\n\nThe file spool is used to retrieve data from a directory of dascore-readable files. It has the same interface as other spools mentioned in the quickstart.\nFor example:\nimport dascore\nfrom dascore import examples as ex\n\n# Get a directory with several files\ndiverse_spool = dascore.get_example_spool('diverse_das')\npath = ex.spool_to_directory(diverse_spool)\n\n# Create a spool for interacting with the files in the directory.\nspool = (\n  dascore.spool(path)\n  .select(network='das2')  # sub-select das2 network\n  .select(time=(None, '2022-01-01'))  # unselect anything after 2022\n  .chunk(time=2, overlap=0.5)  # change the chunking of the patches\n)\n\n# Iterate each patch and do something with it\nfor patch in spool:\n  ...\n\n\n\nThe ‘DirectoryIndexer’ is used to track the contents of a directory which contains fiber data. It creates a small, hidden HDF index file at the top of the directory which can be efficiently queried for retrieving data (it is used internally by the DirectorySpool) or to ask questions about the data archive.\nimport dascore\nfrom dascore.io.indexer import DirectoryIndexer\nfrom dascore import examples as ex\n\n# Get a directory with several files\ndiverse_spool = dascore.get_example_spool('diverse_das')\npath = ex.spool_to_directory(diverse_spool)\n\n# Create an indexer and update the index. This will include any new files\n# with timestamps newer than the last update, or create a new HDF index file\n# if one does not yet exist.\nindexer = DirectoryIndexer(path).update()\n\n# get the contents of the directory's files\ndf = indexer.get_contents()\n\n# This dataframe can be used to ascertain data availability, detect gaps, etc."
  },
  {
    "objectID": "contributing/testing.html",
    "href": "contributing/testing.html",
    "title": "DasCore",
    "section": "",
    "text": "DASCore’s test suite is run with pytest. While in the base dascore repo (and after installing DASCore for development) invoke pytest from the command line:\n\npytest tests\nYou can also use the cov flags to check coverage.\n\npytest tests --cov dascore --cov-report term-missing\nIf you would like to test the IU modules it can be done like so:\n\npytest tests/test_io\nOr a particular IO module:\n\npytest tests/test_io/test_dasdae.py\nPytest is highly configurable and has some rather useful flags such as -s, -x, and –pdb (especially with pdbpp).\n\n\nTests should go into the tests/ folder, which mirrors the structure of the main package. For example, if you are writing tests for dascore.Patch, whose class definition is located in dascore/core/patch it should go in tests/test_core/test_patch.py.\nIn general, tests should be grouped together in classes. Fixtures go as close as possible to the test(s) that need them, going from class, module, and conftest fixtures. Checkout the pytest documentation for a review on fixtures (and why to use them)."
  },
  {
    "objectID": "contributing/new_format.html",
    "href": "contributing/new_format.html",
    "title": "DasCore",
    "section": "",
    "text": "Several steps are required to add support for a new file format. To demonstrate, imagine adding support for a format called jingle which conventionally uses a file extension of jgl.\n\n\nFirst, we create a new io module called ‘jingle’ in DASCore’s io module (dascore/io/jingle). Make sure there is a __init__.py file in this module whose docstring describes the format.\ncontents of dascore/io/jingle/__init__.py\n\"\"\"\nJingle format support module.\n\nJingle is a really cool new format which supports all the \"bells\" and whistles\npeople need for working with das data.\n\nExamples\n--------\nimport dascore as dc\n\njingle = dc.read('path_to_file.jgl')\n\"\"\"\nNext, create a core.py file in the new module (dascore/io/jingle/core.py). Start by creating a class called JingleIO, or better yet, JingleIOV1, which subclasses `dascore.io.core.FiberIO. Then all you need to do is implement the supported methods.\nContents of dascore/io/jingle/core.py\n\"\"\"\nCore module for jingle file format support.\n\"\"\"\nimport dascore.exceptions\nfrom dascore.io.core import FiberIO\n\n\nclass JingleV1(FiberIO):\n    \"\"\"\n    An IO class supporting version 1 of the jingle format.\n    \"\"\"\n    # you must specify the format name using the name attribute\n    name = 'jingle'\n    # you can also define which file extensions are expected like so.\n    # this will speed up determining the formats of files if not specified.\n    preferred_extensions = ('jgl',)\n    # also specifying a version is good practice so when version 2 is released\n    # you can just make another class in the same module named JingleV2.\n    version = '1'\n\n    def read(self, path, jingle_param=1, **kwargs):\n        \"\"\"\n        Read should take a path and return a patch or sequence of patches.\n\n        It can also define its own parameters, and should always accept kwargs.\n        If the format supports partial reads, these should be implemented as well.\n        \"\"\"\n\n    def get_format(self, path):\n        \"\"\"\n        Used to determine if path is a supported jingle file.\n\n        Returns a tuple of (format_name, file_version) if the file is a\n        supported jingle file, else return False or raise a\n        dascore.exceptions.UnknownFiberFormat exception.\n        \"\"\"\n\n    def scan(self, path):\n        \"\"\"\n        Used to gather metadata about a file without reading in the whole file.\n\n        This should return a list of dictionaries with the keys/types specified\n        by dascore.core.schema.PatchFileSummary\n        \"\"\"\n\n    def write(self, patch, path, **kwargs):\n        \"\"\"\n        Write a patch or spool back to disk in the jingle format.\n        \"\"\"\nAll 4 methods are optional; some formats will only support reading, others only writing. If is_format is not implemented the format will not be auto-detectable.\n\n\n\nNext we need to write tests for the format (you weren’t thinking of skipping this step were you!?). The tests should go in: test/test_io/test_jingle.py. The hardest part is finding a small (typically no more than 1-2 mb) file to include in the test suite. Assuming you have found, or manufactured, such a file, it should go into dasdae’s data repo. Simply clone the repo, add you file format, and push back to master or open a PR on a separate branch to include the data file.\nNext, add your file to dascore’s data registry (dascore/data_registry.txt). You will have to get the sha256 hash of your test file, for that you can simply use Pooch’s hash_file function, and you can create the proper download url using the other entries as examples.\nThe name, hash, and url might look something like this:\njingle_test_file.jgl\n12e087d2c1cd08c9afd18334e17e21787be0b646151b39802541ee11a516976a\nhttps://github.com/dasdae/test_data/raw/master/das/jingle_test_file.jgl\nThen your test file might look like this:\ncontents of tests/test_io/test_jingle.py\n\nimport pytest\n\nimport dascore\nfrom dascore.utils.downloader import fetch\n\n\nclass TestJingleIO:\n    \"\"\"Tests for jingle IO format.\"\"\"\n\n    @pytest.fixture(scope='class')\n    def jingle_file_path(self):\n        \"\"\"Return the path to the test jingle file.\"\"\"\n        path = fetch(\"jingle_test_file.jgl\")\n        return path\n\n    @pytest.fixture(scope='class')\n    def jingle_patch(self, jingle_file_path):\n        \"\"\"Read the jingle test data\"\"\"\n        return dascore.read(jingle_file_path, file_format='jingle')[0]\n\n    def test_read(self, jingle_file_path):\n        \"\"\"Ensure the test file can be read.\"\"\"\n        # fetch will download the file (if not already downloaded) and\n        # test read without specifying format\n        out1 = dascore.read(jingle_file_path)\n        # next assert things about the output, maybe len, attributes etc.\n        assert len(out1) == 1\n        # test read specifying format\n        out2 = dascore.read(jingle_file_path, file_format='jingle')\n        ...\n\n    def test_write(self, tmp_path_factory, jingle_patch):\n        \"\"\"Ensure jingle format can be written.\"\"\"\n        out_path = tmp_path_factory.mktemp('jingle') / 'jingle_test.jgl'\n        jingle_patch.io.write(jingle_patch, out_path)\n        # make sure the new file exists and has a size\n        assert out_path.exists()\n        assert out_path.stat().size > 0\n\n    def test_scan(self, jingle_file_path):\n        \"\"\"Tests for scanning a jingle file\"\"\"\n        scan = dascore.scan(jingle_file_path)\n        assert len(scan) == 1\n\n    def test_is_format(self, jingle_file_path):\n        \"\"\"Tests for automatically determining a jingle format.\"\"\"\n        file_format, version = dascore.get_format(jingle_file_path)\n        assert file_format.lower() == 'jingle'\n\n\n\nNow that the Jingle format support is implemented and tested, the final step is to register the jingle core module in DASCore’s entry points. This is done under the [project.entry-points.”dascore.fiber_io”] section in dascore’s pyproject.toml file. For example, after adding jingle the pyproject.toml section might look like this:\n[project.entry-points.\"dascore.fiber_io\"]\nTERRA15__V4 = \"dascore.io.terra15.core:Terra15FormatterV4\"\nPICKLE = \"dascore.io.pickle.core:PickleIO\"\nWAV = \"dascore.io.wav.core:WavIO\"\nDASDAE__V1 = \"dascore.io.dasdae.core:DASDAEV1\"\nJINGLE__V1 = \"dascore.io.jingle.core:JingleV1\"\nNote the name of the format is always on the left side of a double underscore if the version is included (recommended)."
  },
  {
    "objectID": "contributing/code_of_conduct.html",
    "href": "contributing/code_of_conduct.html",
    "title": "DasCore",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders via private message on github.\nAll complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "contributing/contributing.html",
    "href": "contributing/contributing.html",
    "title": "DasCore",
    "section": "",
    "text": "Contributions to DASCore are welcomed and appreciated. Before contributing please be sure to read and understand our code of conduct.\nDevelopment Installation\nTesting\nAdding Support for a New File Format\n\n\n\n\nWe create new features or bug fixes in their own branches and merge them into master via pull requests. We may switch to a more complex branching model if the need arises.\nIf substantial new features have been added since the last release we will bump the minor version. If only bug fixes/minor changes have been made, only the patch version will be bumped. Like most python projects, we loosely follow semantic versioning in terms that we will not bump the major version until DASCore is more stable.\n\n\n\nDASCore uses Black and flake8 for code linting. If you have properly installed DASCore’ pre-commit hooks they will be invoked automatically when you make a git commit. If any complaints are raised simply address them and try again.\nAs a reminder, you can install pre-commit hooks like so:\npip install pre-commit\npre-commit install\nThen run all the hooks like this:\npre-commit run --all\n\n\n\nUse numpy style docstrings. All public code (doesn’t start with a _) should have a “full” docstring but private code (starts with a _) can have an abbreviated docstring.\nDASCore makes extensive use of Python 3’s type hints. Use them to annotate any public functions/methods.\nHere is an example:\nfrom typing import Optional, Union\n\nimport dascore\nfrom dascore.constants import PatchType\n\n\n# example public Patch function\n@dascore.patch_function()\ndef example_func(patch: PatchType, to_add: Optional[Union[int, float]]) -> PatchType:\n    \"\"\"\n    A simple, one line explanation of what this function does.\n\n    Additional details which might be useful, and are not limited to one line.\n    In fact, they might span several lines, especially if the author of the\n    docstring tends to include more details than needed.\n\n    Parameters\n    ----------\n    patch\n        A description of this parameter\n    to_add\n        A description of this parameter\n\n    Returns\n    -------\n    If needed, more information about what this function returns. You shouldn't\n    simply specify the type here since that is already given by the type annotation.\n    If the returned object is self-explanatory feel free to skip this section.\n\n    Examples\n    --------\n    >>> # Examples are included in the doctest style\n    >>> import dascore\n    ... pa = dascore.get_example_patch()\n    ...\n    ... out = example_func(pa)\n    \"\"\"\n    data = pa.data\n    if to_add is not None:\n        data += data\n    return patch.__class__(data=data, attrs=patch.atts, coords=patch.coords)\n\n\n# example private function\ndef _recombobulate(df, arg1, arg2):\n    \"\"\"\n    A private function can have a simple (multi-line) snippet and doesn't need as\n    much detail or type hinting as a public function.\n    \"\"\"\n\n\n\n\nPrefer pathlib.Path to strings when working with paths. However, when dealing with many many files (e.g., indexers) strings may be preferred for efficiency.\n\n\n\nColumn names should be snake_cased whenever possible.\nAlways access columns with getitem and not getattr (ie use df['column_name'] not df.column_name).\nPrefer creating a new DataFrame/Series to modifying them inplace. Inplace modifications should require opting in (usually through an inplace key word argument)."
  },
  {
    "objectID": "contributing/documentation.html",
    "href": "contributing/documentation.html",
    "title": "DasCore",
    "section": "",
    "text": "The documentation can be built using the script called “make_docs.py” in the scripts directory. If you have followed the development installation instructions, all the required dependencies should be installed. You will also need to install pandoc using conda or the offical installation.\n\npython scripts/make_docs.py\nThe docs can then be accessed by double-clicking on the newly created html index at docs/_build/html/index.html."
  },
  {
    "objectID": "contributing/documentation.html#contributing-to-the-documentation",
    "href": "contributing/documentation.html#contributing-to-the-documentation",
    "title": "DasCore",
    "section": "Contributing to the documentation",
    "text": "Contributing to the documentation\nThe documentation is primarily done in markdown but easily converted to jupyter notebooks using jupytext.\nMarkdown files should go into docs/markdown. If the markdown file contains python code which should be tested (most python code should be tested), the file should go in docs/markdown/executable. The executable code uses the myst markdown flavor. For an example see the intro page source code."
  },
  {
    "objectID": "contributing/dev_install.html",
    "href": "contributing/dev_install.html",
    "title": "DasCore",
    "section": "",
    "text": "The following steps are needed to set up DASCore for development:\n\n\n\ngit clone https://github.com/dasdae/dascore\ncd dascore\n\n\n\nMake sure to pull all of the latest git tags.\nNOTE: You may need to do this periodically to keep tags in sync.\n\ngit pull origin master --tags\n\n\n\nCreate and activate a virtual environment so DASCore will not mess with the base (or system) python installation.\nIf you are using Anaconda, simply use the environment provided:\n\nconda env create -f environment.yml\nconda activate dascore\n\n\n\n\npip install -e .[test,docs]\n\n\n\ndascore uses several pre-commit hooks to ensure the code stays tidy. Please install and use them!\n\npre-commit install -f"
  }
]